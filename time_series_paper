\title{A Diffusion Model for Regular Time Series Generation from Irregular Data with Completion and Masking}

\begin{abstract}
  Generating realistic time series data is critical for applications in healthcare, finance, and science. However, irregular sampling and missing values present significant challenges. While prior methods address these irregularities, they often yield suboptimal results and incur high computational costs. Recent advances in regular time series generation, such as the diffusion-based ImagenTime model, demonstrate strong, fast, and scalable generative capabilities by transforming time series into image representations, making them a promising solution. However, extending ImagenTime to irregular sequences using simple masking introduces ``unnatural'' neighborhoods, where missing values replaced by zeros disrupt the learning process. To overcome this, we propose a novel two-step framework: first, a Time Series Transformer completes irregular sequences, creating natural neighborhoods; second, a vision-based diffusion model with masking minimizes dependence on the completed values. This approach leverages the strengths of both completion and masking, enabling robust and efficient generation of realistic time series. Our method achieves state-of-the-art performance, achieving a relative improvement in discriminative score by $70\%$ and in computational cost by $85\%$. Code is at \url{https://github.com/azencot-group/ImagenI2R}.
\end{abstract}

\section{Introduction}

Time series data is essential in fields such as healthcare, finance, and science, supporting critical tasks like forecasting trends, detecting anomalies, and analyzing patterns~\cite{han2019review, ismail2019deep, lim2021time}. Beyond direct analysis, generating synthetic time series has become increasingly valuable for creating realistic proxies of private data, testing systems under new scenarios, exploring ``what-if'' questions, and balancing datasets for training machine learning models~\cite{brophy2023generative, nochumsohn2025data}. The ability to generate realistic sequences enables deeper insights and robust applications across diverse domains.

In practice, however, time series is often \emph{irregular}, with unevenly spaced measurements and missing values. These irregularities arise from limitations in data collection processes, such as sensor failures, inconsistent sampling, or interruptions in monitoring systems~\cite{kidger2020neural, ren2024learning}. This irregularity poses a unique challenge for generating regular time series---where intervals are consistent, and the data follows the same distribution as if it were regularly observed~\cite{jeon2022gt}. The main goal of this paper is to generate regular sequences by training on irregularly-sampled time series using deep neural networks.


% existing approaches, their shortcomings
The synthesis of regular time series from irregular ones is a fundamental challenge, yet existing approaches remain scarce, with notable examples being GT-GAN and KoVAE~\cite{jeon2022gt, naiman2024generative}. Unfortunately, these methods suffer from several limitations. First, they rely on generative adversarial networks (GANs) and variational autoencoders (VAEs), which have recently been surpassed in performance by diffusion-based tools \cite{coletta2023constrained, yuan2024diffusion, naiman2024utilizing}. Second, both GT-GAN and KoVAE utilize a computationally-demanding preprocessing step based on neural controlled differential equations (NCDEs)~\cite{kidger2020neural}, rendering these methods impractical for long time series. For instance, KoVAE requires $\approx 6.5 \times$ more training time in comparison to our approach (See Fig.~\ref{fig:disc_time_graph}). Third, these methods inherently assume that the data, completed by NCDE, accurately reflects the true underlying distribution, which can introduce catastrophical errors when this assumption fails. In particular, their performance lags far behind that of models trained on regular time series, with state-of-the-art results on irregular discriminative benchmarks being, on average, $540\%$ worse than those on regular benchmarks.


% our approach
To address these shortcomings, we base our approach on a recent diffusion model for time series, \emph{ImagenTime}~\cite{naiman2024utilizing}. This method maps time series data to images, enabling the use of powerful vision-based diffusion neural architectures. Leveraging a vision-based diffusion generator offers a significant advantage: regular time series can be generated from irregular ones using a straightforward masking mechanism. Specifically, missing values in the series are seamlessly ignored during the denoising process in training, akin to techniques used in image inpainting tasks~\cite{lugmayr2022repaint, corneanu2024latentpaint}.

However, while this straightforward masking approach is simple and achieves strong results (see Tab.~\ref{tab:ablation}), we identify a significant limitation. Missing values in the time series are mapped to zeros in the image, resulting in ``unnatural'' neighborhoods that mix valid and invalid information. This can pose challenges for diffusion backbones, such as U-Nets with convolutional blocks, where the convolution kernels are not inherently masked and may inadvertently propagate errors from these artificial neighborhoods. To address this issue, we propose a two-step generation process. In the first step, we complete the irregular series using our adaptation of an efficient Time Series Transformer (TST) approach~\cite{zerveas2021transformer}, significantly reducing computational overhead and enabling the generation of long time series. In the second step, we apply the straightforward masking approach described earlier. Crucially, this combination of completion and masking allows the model to learn from ``natural'' image neighborhoods while mitigating the reliance on fully accurate completed information through the use of masked loss minimization. Overall, our approach, built on a vision diffusion backbone, enables effective modeling of long time series while making minimal assumptions about pre-completed data, resulting in significantly efficient and improved generation performance.


%  evaluation
We conduct a comprehensive evaluation of our approach on standard irregular time series benchmarks, benchmarking it against state-of-the-art methods. Our model consistently demonstrates superior generative performance, effectively bridging the gap between regular and irregular settings. Furthermore, we extend the evaluation to medium-, long- and ultra-long-length sequence generation, assessing performance across $12$ datasets and $12$ tasks. The results highlight the robustness and efficiency of our method, achieving consistent improvements over existing approaches. Our key contributions are summarized as follows:
\vspace{-1mm}
\begin{enumerate}
    \item We introduce a novel generative model for irregularly-sampled time series, leveraging vision-based diffusion approaches to efficiently and effectively handle sequences ranging from short to long lengths.
    \item In contrast to existing methods that assume completed information is drawn from the data distribution, we treat it as a weak conditioning signal and directly optimize on the observed signal using a masking strategy.
    \item Our approach achieves state-of-the-art performance across multiple generative tasks, delivering an average improvement of $70\%$ in discriminative benchmarks while reducing computational requirements by $85\%$ relative to competing methods.
\end{enumerate}

\section{Background}
\paragraph{Problem statement.} We learn the underlying distribution of time series data from irregularly sampled observations and generating regular time series from it. Specifically, given a set of irregularly sampled sequences, our goal is to learn a model that approximates the true data distribution \( p_{\text{data}}(x_{1:T}) \) and enables sampling of complete time series \( x_{1:T} \) from the learned distribution \( p_\theta(x_{1:T}) \). Formally, we consider a dataset of irregularly sampled time series, represented as \( \{ x_{t_1:t_n}^j \}_{j=1}^N \), where each sequence consists of observations at non-uniform time steps \( t_1:t_n = [t_1, t_2, \dots, t_n] \) with \( t_1 \geq 1 \) and \( t_n \leq T \). The challenge is to leverage these incomplete sequences to model the full distribution and generate realistic, regularly sampled time series that align with the true data distribution.

\paragraph{ImagenTime}\hspace{-3mm} employs the delay embedding transformation to map time series data into images, enabling their processing with powerful vision-based diffusion models~\cite{naiman2024utilizing}. Given an input multivariate regular time series $x_{1:T} \in \mathbb{R}^{d \times T}$ with $d$ features and length $T$, the delay embedding constructs an image $x_\text{img} \in \mathbb{R}^{d \times w \times h}$ by placing $x$'s values over the columns of $x_\text{img}$ per channel, where $w, h$ are user-defined parameters. During training, noise is added to the image $x_\text{img}$ at different timesteps, forming $x_\text{img}(t)$. The diffusion model, parameterized by $s_\theta$, learns to denoise these images by approximating the score function $s_\theta(x_\text{img}, t)$. Inference begins with a noise sample $x_\text{img}(T) \sim \mathcal{N}(0, I)$. This sample is iteratively denoised using the learned score function to produce a cleaned image $x_\text{img}(0)$. The inverse delay embedding transform is then applied to $x_\text{img}(0)$, reconstructing the original time series $\tilde{x}_{1:T}$. Importantly, the inverse transformation of delay embedding is inherently non-unique, as the time series values are repeated within the image representation, suggesting various designs can be considered as we discuss in Sec.~\ref{sec:method}. Finally, a crucial advantage of ImagenTime is its effectivity in handling long series, e.g., a time series of length $65k$ is transformed to an image of size $256\times 256$.

\paragraph{TST}\hspace{-3mm} leverages the self-attention mechanism of Transformers to model temporal dependencies and long-range interactions in time-series data effectively. Unlike traditional sequence models such as RNNs or LSTMs, TST processes the entire sequence simultaneously, enabling parallelism and mitigating the vanishing gradient problem. The architecture includes input projection to a higher-dimensional feature space, positional encodings to capture temporal order, and a stack of Transformer encoder layers with flexible normalization and activation options. TST is particularly suitable for imputation tasks, as it can handle irregularly-sampled data and missing values through explicit masking and preprocessing. Additionally, its self-attention mechanism inherently supports long sequences, making it robust for capturing global context and dependencies in extended time-series data. By eliminating the need for computationally expensive preprocessing techniques, such as calculating coefficients for cubic splines or other interpolation methods, TST achieves significant speed advantages while providing a scalable, efficient, and accurate solution for tasks like forecasting, classification, anomaly detection, and imputation.

\section{Method}
While ImagenTime does not address the challenge of irregularly-sampled time series, a simple extension can enable it to generate regularly-sampled time series by training on irregular data. The key idea involves employing a \emph{mask} during the loss computation. This mask ensures that only ``active'' pixels--those corresponding to observed time series values--are considered in the loss calculation, while ``non-active'' pixels, representing missing information, are effectively ignored. This approach enables effective learning from incomplete data while preserving the integrity of the observed information, offering two key advantages: (i) the mask is architecture-agnostic, making it compatible with any diffusion backbone, and (ii) the inference procedure of ImagenTime remains entirely unchanged.
Unfortunately, the straightforward approach has a fundamental limitation: although non-active pixels are ignored during loss computation, they are still processed by the network. In practice, missing values are replaced with zeros, resulting in ``unnatural'' pixel neighborhoods. Specifically, while zeros may occasionally occur in non-zero segments of a time series, their repeated presence is highly unlikely, leading to inconsistencies. In other words, masking is not applied at the architecture level, potentially hindering the effective learning of neural components.
One possible solution to alleviate the phenomenon of unnatural neighborhoods is to implement masking at the kernel level, but this would require modifications tailored to each neural architecture, thereby restricting the approach's flexibility and its straightforward applicability across different models. For instance, while our work employs a convolutional U-Net, recent transformer-based architectures have emerged as highly effective diffusion backbones~\cite{peebles2023scalable}. Accommodating such diversity in architectures would require a more generalized solution.
To construct more natural pixel neighborhoods while remaining architecture-agnostic, we take inspiration from the two-stage pipelines of GT-GAN and KoVAE~\cite{jeon2022gt,naiman2024generative}. Our method likewise uses a two-step training scheme. First, we complete missing values in irregularly sampled time series using TST~\cite{zerveas2021transformer} to obtain a regularly sampled sequence. Second, we transform the completed series into an image and apply denoising as in ImagenTime, with one key change: during the loss computation we \emph{mask} the pixels that originated from completion (see App.~\ref{app:loss_function}), following the straightforward masking strategy discussed earlier. In the toy experiment of Sec.~\ref{subsec:neigh}, the completed neighborhood (Fig.~\ref{fig:unnatural_neigh}C) enables learning of consistent kernels (Fig.~\ref{fig:unnatural_neigh}F) and improves score estimation to $0.32$.

We also replicated the synthetic experiment on a real-world \textbf{Stocks} dataset, using a larger convolutional kernel and comparing the two ways to complete irregular neighborhoods: (i) zero filling and (ii) natural-neighbor filling. The Stocks results mirrored the synthetic study, reinforcing our hypothesis that ``unnatural'' neighbors (e.g., zeros) are detrimental. Evaluated by score-estimation loss on active pixels, simple masking delivered only a marginal gain ($0.81 \to 0.79$). Visual inspection likewise matched the synthetic patterns. In contrast, our proposed approach, which avoids zero padding and learning from semantically valid neighborhoods, significantly improved performance, reducing the loss to \textbf{0.29} and yielding more consistent kernel behavior. See App.\ref{sec:unnatural_cont} for visuals.

This combination of \emph{completion + masking} addresses the two primary challenges of irregular sequences. Completion creates natural neighborhoods so convolutional kernels learn from values closer to the true data distribution; masking prevents over-reliance on imputed values by excluding them from the loss, striking a balance between leveraging and mitigating incomplete information. Figure~\ref{fig:arch} illustrates our pipeline: autoencoder pretraining (top), main training (middle), and inference (bottom). $\mathcal{T}$ and $\mathcal{T}^{-1}$ denote the delay-embedding transform and its inverse; the fire and snowflake icons indicate trainable and frozen modules, respectively.


Importantly, we identify limitations with $\mathcal{T}^{-1}$ that was proposed in~\cite{naiman2024utilizing}. Specifically, in their approach, only the first pixel corresponding to each time series value is used for reconstruction. Specifically, if $x_i$ is mapped to multiple image indices, the original method selects the first corresponding pixel in the image for reconstruction. We modify this inverse transformation by aggregating information from all corresponding image indices and computing the average of the associated pixels for each \( x_i \). For a given \( x_{1:L} \in \mathbb{R}^L \), both methods ensure that \( f^{-1}(f(x)) = x \). See Sec.~\ref{subsec:ablation} for an ablation study of these two methods.

\section{Results}
\subsection{Quantitative Evaluation}
Our quantitative evaluations assess missing rate setups of 30\%, 50\%, and 70\%. For example, in the 30\% missing rate case, we randomly omit 30\% of the data in each training sample. Additionally, we extend the standard benchmark, which typically considers a sequence length of 24, to include longer sequences of 96, 768, and 10,920, providing a more comprehensive evaluation across varying temporal scales. We utilize a diverse set of datasets, extending beyond common benchmarks such as Sine, Stock, Energy, and MuJoCo to include additional real-world datasets: ETTh1, ETTh2, ETTm1, ETTm2, Weather, Electricity, KDD-Cup, and Traffic. We compare against the popular TimeGAN approach~\cite{yoon2019time}, adapted to handle irregular data by incorporating the time difference between samples as input. In addition, we also consider the recent GT-GAN~\cite{jeon2022gt} and KoVAE~\cite{naiman2024generative}.


We evaluate the performance of our model using the \emph{discriminative} and \emph{predictive} tasks suggested by~\cite{yoon2019time}. In the discriminative task, we measure the similarity between real and synthetic samples by training a classifier to distinguish between the two, reporting $|0.5 - \text{acc}|$, where $\text{acc}$ is the accuracy of the classifier. For the predictive task, we adopt the ``train on synthetic, test on real'' protocol, where a predictor is trained on synthetic data and tested on real data. The performance is evaluated using the Mean Absolute Error (MAE). We also consider irregular time series metrics: the \emph{Context-FID score}~\cite{jeha2022psa}, which quantifies the similarity in distribution between synthetic and real data, and the \emph{correlation score}~\cite{liao2020conditional}, which evaluates the feature-level relationship between the two datasets. The Context-FID score is computed by encoding both synthetic and real sequences using TS2Vec~\cite{yue2022ts2vec} and calculating the FID score on the representations. The correlation score measures the covariance of features between real and synthetic data, with a focus on assessing their alignment. Full details are provided in App.~\ref{app:metrics}. For all metrics, lower scores are better.

Tab.~\ref{tab:results_short} details the benchmark results for a sequence length of 24. The values represent averages over the $30\%, 50\%$ and $70\%$ missing rates, where the full results are provided in Tab.~\ref{tab:irregular_24}. In general, our approach presents dramatic improvements across all metrics with respect to the second-best approach (typically KoVAE). Following~\cite{naiman2024generative}, we define the relative improvement error as $e_\text{rel} = (e_2-e_1)/e_2$, where $e_2$ is the second-best error and $e_1$ is ours. In this metric, averaged across all datasets, our method improves by $\textbf{74.2\%, 15.0\%, 78.5\%, 62.1\%}$ in the discriminative, predictive, context-FID, and correlation scores, respectively. We also compared our approach to KoVAE in the medium (96) and long (768) lengths on all datasets excluding MuJoCo and electricity. The results appear in Tab.~\ref{tab:results_long},

\subsection{Irregularly-sampled data under noise}
Our work primarily addresses irregularly sampled time series data. However, in real-world scenarios, such data often includes noise due to sensor limitations and inaccuracies. To further enhance our quantitative evaluation framework, we tackle the generative challenges of learning from irregular time series in noisy environments. Specifically, we propose a novel setup to evaluate the model's capability to recover the true underlying distribution from data corrupted by both irregular sampling and Gaussian noise. In this setup, we simulate a 50\% missing rate and inject additive Gaussian noise sampled from a normal distribution $\mathcal{N}(0, \sigma)$, where $\sigma$ corresponds to the specified noise level (e.g., $0.1, 0.15, \text{and } 0.2$). Importantly, this noise is added independently of the original data distribution or scale. The evaluation is conducted on sequences of length 24 across four datasets: Weather, Etth1, Stocks. and Energy.  Following the discriminative and predictive evaluation protocols for $50\%$ missing rate described earlier, we compare our approach against the most recent state-of-the-art method, KoVAE. Tab.~\ref{tab:gaussian_noise} presents the results. For each noise rate (N/R), we report the discriminative (Disc.) and predictive (Pred.) scores, where lower values indicate better performance. Our method consistently outperforms KoVAE, achieving significant improvements. Specifically, we observe an average relative improvement of \textbf{26.8\%} in the discriminative score and \textbf{50.3\%} in the predictive score across all datasets and noise levels.

\subsection{Ablation Studies}
To better understand the contributions of each component in our proposed architecture, we conducted a series of ablation studies. We explore each of the components in our approach separately, and, additionally, we modify recent approaches to include our completion strategy. Specifically, we consider the following models: (i) KoVAE + TST, where the NCDE module in KoVAE is replaced by TST, as in our approach. (ii) TimeAutoDiff~\cite{suh2024timeautodiff} + TST (iii): TransFusion \cite{sikder2023transfusion}
 + TST; The latter two baselines are diffusion-based models designed for regular time series but not specifically for irregular time series data. Therefore, we used the TST module to impute the missing values. (iv) Mask Only, where the TST autoencoder is removed, and we only apply the masking mechanism. In this setup, missing values are imputed using unnatural neighbors by filling them with zeros. (v) Ours Without Masking, where we leverage TST to complete missing values, and training is performed without masking. (vi) Our approach.



We quantitatively evaluated each model under the same experimental conditions and show the results in Tab.~\ref{tab:ablation}. To provide an extensive analysis, our tests include several missing rates ($30\%, 50\%, \text{and } 70\%$) using two datasets, Energy and Stock. Further, we measured the performance across different sequence lengths ($24, 96, \text{and } 768$). Our findings show that the combination of TST-based completion and masking yields superior performance compared to all other setups. Specifically, the Mask Only and Ours (Without Masking) setups showed significant limitations in capturing the true data distribution, while the replacement of NCDE with TST (KoVAE + TST) fell short in comparison to our proposed architecture. In particular, our results reveal that replacing the NCDE imputation component in KoVAE with the TST imputation mechanism is not the primary factor driving the significant improvements achieved by our method. Moreover, even when employing a powerful time series diffusion-based model like TransFusion combined with TST-based
imputation, performance significantly degrades, struggling to capture the true distribution of the regular data. Overall, these results highlight the critical role of masking during the diffusion process and the importance of leveraging completion as a guide rather than a direct substitute for the true distribution.

We also ablate the impact of different image transformations on model performance, evaluating four methods: vanilla folding (reshapes a sequence into a fixed-size matrix with zero-padding), Gramian Angular Field, basic delay embedding, and our proposed inverse delay embedding (see App.~\ref{app:ts2img}). Results in Tab.~\ref{tab:image_transformations} and Tab.~\ref{tab:ablation_de} show that geometric approaches—vanilla folding and our enhanced DE—better suit our method due to their structural clarity, where each pixel maps directly to a time point, facilitating mask usage and improving both discriminative and predictive performance. In contrast, GAF does not scale well to long sequences due to large image size. Our inverse transform also outperforms the original inverse of ImagenTime~\cite{naiman2024utilizing}.

We additionally conducted an extensive ablation study comparing a variety of completion strategies. These included simple methods such as, Gaussian noise (GN), zero-filling, linear interpolation (LI), and polynomial interpolation (PI); probabilistic techniques like stochastic imputation (SI) (sampling from a Gaussian distribution fitted to the non-NaN values in each slice); and more advanced learning-based approaches, including NCDE, CSDI~\cite{tashiro2021csdi}, and our proposed Time Series Transformer (TST) completion. Our results in Tab.~\ref{tab:ablation_50_combined} show that when the neighbors are not natural—such as in the case of zero completion or Gaussian noise completion—the model struggles more to generate data that closely follows the true distribution. In contrast, when using more natural completions (e.g., polynomial, stochastic imputation, NCDE, CSDI, TST), the model consistently obtains very good results. This confirms that generating natural neighborhoods indeed enhances the generative quality without making the model completely reliant on the imputation quality.